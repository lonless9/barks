//! RDD transformation implementations.

use crate::operations::RddDataType;
use crate::rdd::{CogroupedRdd, JoinedRdd, ShuffledRdd, SortedRdd};
use crate::shuffle::{Aggregator, Partitioner, ReduceAggregator};
use crate::traits::{Data, IsRdd, RddBase};
use std::sync::Arc;

/// An extension trait for RDDs of key-value pairs.
pub trait PairRdd<K: Data, V: Data>
where
    Self: Sized,
{
    /// Groups values by key and applies a reduction function.
    /// This is a wide transformation that triggers a shuffle.
    fn reduce_by_key(
        self,
        reduce_func: fn(V, V) -> V,
        partitioner: Arc<dyn Partitioner<K>>,
    ) -> ShuffledRdd<K, V, V>;

    /// Groups all values for a key into a single sequence.
    /// This is a wide transformation that triggers a shuffle.
    fn group_by_key(
        self,
        partitioner: Arc<dyn Partitioner<K> + 'static>,
    ) -> ShuffledRdd<K, V, Vec<V>>;

    /// Return an RDD containing all pairs of elements with matching keys in `self` and `other`.
    fn join<W: Data>(
        self,
        other: Arc<dyn crate::traits::RddBase<Item = (K, W)>>,
        partitioner: Arc<dyn Partitioner<K> + 'static>,
    ) -> JoinedRdd<K, V, W>;

    /// Return an RDD with the elements sorted by key.
    fn sort_by_key(self, ascending: bool) -> SortedRdd<K, V>
    where
        K: Ord + std::fmt::Debug;

    /// Return an RDD that groups data from both RDDs by key.
    /// This is the foundation for join operations.
    fn cogroup<W: Data>(
        self,
        other: Arc<dyn crate::traits::RddBase<Item = (K, W)>>,
        partitioner: Arc<dyn Partitioner<K> + 'static>,
    ) -> CogroupedRdd<K, V, W>;

    /// Combine values with the same key using a custom aggregator.
    fn combine_by_key<C: Data>(
        self,
        aggregator: Arc<dyn crate::shuffle::Aggregator<K, V, C>>,
        partitioner: Arc<dyn Partitioner<K>>,
    ) -> ShuffledRdd<K, V, C>;
}

impl<K, V> PairRdd<K, V> for crate::rdd::DistributedRdd<(K, V)>
where
    K: Data,
    V: Data,
    (K, V): RddDataType,
{
    fn reduce_by_key(
        self,
        reduce_func: fn(V, V) -> V,
        partitioner: Arc<dyn Partitioner<K>>,
    ) -> ShuffledRdd<K, V, V> {
        let aggregator = Arc::new(ReduceAggregator::new(reduce_func));
        // The ID of a new RDD is typically derived or generated by the context.
        // Here we use a placeholder.
        let new_rdd_id = self.id().saturating_add(1); // Simplistic ID generation
        ShuffledRdd::new(new_rdd_id, Arc::new(self), aggregator, partitioner)
    }

    fn group_by_key(
        self,
        partitioner: Arc<dyn Partitioner<K> + 'static>,
    ) -> ShuffledRdd<K, V, Vec<V>> {
        let aggregator = Arc::new(crate::shuffle::GroupByKeyAggregator::new());
        let new_rdd_id = self.id().saturating_add(2);
        ShuffledRdd::new(new_rdd_id, Arc::new(self), aggregator, partitioner)
    }

    fn join<W: Data>(
        self,
        other: Arc<dyn RddBase<Item = (K, W)>>,
        partitioner: Arc<dyn Partitioner<K> + 'static>,
    ) -> JoinedRdd<K, V, W> {
        let new_rdd_id = self.id().saturating_add(other.id()).saturating_add(1);
        JoinedRdd::new(new_rdd_id, Arc::new(self), other, partitioner)
    }

    fn sort_by_key(self, ascending: bool) -> SortedRdd<K, V>
    where
        K: Ord + std::fmt::Debug,
    {
        let new_rdd_id = self.id().saturating_add(4);
        let num_partitions = self.num_partitions() as u32;

        // Sample the RDD to determine range bounds for proper partitioning
        let sample_size = (num_partitions as usize * 20).max(100); // 20 samples per partition, minimum 100
        let parent_rdd: Arc<dyn crate::traits::RddBase<Item = (K, V)>> = Arc::new(self.clone());
        let sample_keys = crate::rdd::sorted_rdd::sample_keys_for_sorting(&parent_rdd, sample_size);

        let partitioner = Arc::new(crate::shuffle::RangePartitioner::from_sample(
            num_partitions,
            sample_keys,
        ));

        SortedRdd::new(new_rdd_id, Arc::new(self), partitioner, ascending)
    }

    fn cogroup<W: Data>(
        self,
        other: Arc<dyn RddBase<Item = (K, W)>>,
        partitioner: Arc<dyn Partitioner<K> + 'static>,
    ) -> CogroupedRdd<K, V, W> {
        let new_rdd_id = self.id().saturating_add(other.id()).saturating_add(2);
        CogroupedRdd::new(new_rdd_id, Arc::new(self), other, partitioner)
    }

    fn combine_by_key<C: Data>(
        self,
        aggregator: Arc<dyn Aggregator<K, V, C>>,
        partitioner: Arc<dyn Partitioner<K>>,
    ) -> ShuffledRdd<K, V, C> {
        let new_rdd_id = self.id().saturating_add(3);
        ShuffledRdd::new(new_rdd_id, Arc::new(self), aggregator, partitioner)
    }
}
