//! RDD transformation implementations.

use crate::rdd::{CogroupedRdd, JoinedRdd, ShuffledRdd, SortedRdd};
use crate::shuffle::{Aggregator, Partitioner, ReduceAggregator};
use crate::traits::{Data, IsRdd, Pair, RddBase, RddResult};
use std::sync::Arc;

/// Type aliases for complex return types to satisfy clippy's type complexity requirements
///
/// Type alias for joined RDD return type
type JoinedRddResult<K, V, W> = Arc<JoinedRdd<K, V, W>>;

/// Type alias for shuffled RDD return type with same input and output value types
type ShuffledRddSameType<K, V> = Arc<ShuffledRdd<K, V, V>>;

/// Type alias for shuffled RDD return type with grouped values
type ShuffledRddGrouped<K, V> = Arc<ShuffledRdd<K, V, Vec<V>>>;

/// Type alias for shuffled RDD return type with custom combined type
type ShuffledRddCombined<K, V, C> = Arc<ShuffledRdd<K, V, C>>;

/// Type alias for sorted RDD return type
type SortedRddResult<K, V> = Arc<SortedRdd<K, V>>;

/// Type alias for cogrouped RDD return type
type CogroupedRddResult<K, V, W> = Arc<CogroupedRdd<K, V, W>>;

/// Type alias for reduce function signature
type ReduceFunction<V> = fn(V, V) -> V;

/// An RDD of key-value pairs.
pub trait PairRdd: RddBase
where
    Self::Item: Pair,
{
    /// Return an RDD containing all pairs of elements with matching keys in `self` and `other`.
    fn join<W: Data>(
        self: Arc<Self>,
        other: Arc<dyn RddBase<Item = (<Self::Item as Pair>::Key, W)>>,
        partitioner: Arc<dyn Partitioner<<Self::Item as Pair>::Key>>,
    ) -> JoinedRddResult<<Self::Item as Pair>::Key, <Self::Item as Pair>::Value, W>;

    /// Groups values by key and applies a reduction function.
    fn reduce_by_key(
        self: Arc<Self>,
        reduce_func: ReduceFunction<<Self::Item as Pair>::Value>,
        partitioner: Arc<dyn Partitioner<<Self::Item as Pair>::Key>>,
    ) -> ShuffledRddSameType<<Self::Item as Pair>::Key, <Self::Item as Pair>::Value>;

    /// Groups all values for a key into a single sequence.
    fn group_by_key(
        self: Arc<Self>,
        partitioner: Arc<dyn Partitioner<<Self::Item as Pair>::Key>>,
    ) -> ShuffledRddGrouped<<Self::Item as Pair>::Key, <Self::Item as Pair>::Value>;

    /// Return an RDD with the elements sorted by key.
    fn sort_by_key(
        self: Arc<Self>,
        ascending: bool,
    ) -> SortedRddResult<<Self::Item as Pair>::Key, <Self::Item as Pair>::Value>
    where
        <Self::Item as Pair>::Key: Ord + std::fmt::Debug;

    /// Return an RDD that groups data from both RDDs by key.
    /// This is the foundation for join operations.
    fn cogroup<W: Data>(
        self: Arc<Self>,
        other: Arc<dyn RddBase<Item = (<Self::Item as Pair>::Key, W)>>,
        partitioner: Arc<dyn Partitioner<<Self::Item as Pair>::Key>>,
    ) -> CogroupedRddResult<<Self::Item as Pair>::Key, <Self::Item as Pair>::Value, W>;

    /// Combine values with the same key using a custom aggregator.
    fn combine_by_key<C: Data>(
        self: Arc<Self>,
        aggregator: Arc<dyn Aggregator<<Self::Item as Pair>::Key, <Self::Item as Pair>::Value, C>>,
        partitioner: Arc<dyn Partitioner<<Self::Item as Pair>::Key>>,
    ) -> ShuffledRddCombined<<Self::Item as Pair>::Key, <Self::Item as Pair>::Value, C>;

    /// The safe, internal method for creating shuffle map tasks.
    /// This is the key to removing `unsafe`.
    fn create_shuffle_map_tasks(
        &self,
        shuffle_id: u32,
        num_partitions: u32,
    ) -> RddResult<Vec<Box<dyn crate::distributed::task::Task>>>;
}

impl PairRdd for crate::rdd::DistributedRdd<(String, i32)> {
    fn reduce_by_key(
        self: Arc<Self>,
        reduce_func: fn(i32, i32) -> i32,
        partitioner: Arc<dyn Partitioner<String>>,
    ) -> Arc<ShuffledRdd<String, i32, i32>> {
        let aggregator = Arc::new(ReduceAggregator::new(reduce_func));
        // The ID of a new RDD is typically derived or generated by the context.
        // Here we use a placeholder.
        let new_rdd_id = self.id().saturating_add(1); // Simplistic ID generation
        Arc::new(ShuffledRdd::new(new_rdd_id, self, aggregator, partitioner))
    }

    fn group_by_key(
        self: Arc<Self>,
        partitioner: Arc<dyn Partitioner<String>>,
    ) -> Arc<ShuffledRdd<String, i32, Vec<i32>>> {
        let aggregator = Arc::new(crate::shuffle::GroupByKeyAggregator::new());
        let new_rdd_id = self.id().saturating_add(2);
        Arc::new(ShuffledRdd::new(new_rdd_id, self, aggregator, partitioner))
    }

    fn join<W: Data>(
        self: Arc<Self>,
        other: Arc<dyn RddBase<Item = (String, W)>>,
        partitioner: Arc<dyn Partitioner<String>>,
    ) -> Arc<JoinedRdd<String, i32, W>> {
        let new_rdd_id = self.id().saturating_add(other.id()).saturating_add(1);
        Arc::new(JoinedRdd::new(new_rdd_id, self, other, partitioner))
    }

    fn sort_by_key(self: Arc<Self>, ascending: bool) -> Arc<SortedRdd<String, i32>> {
        let new_rdd_id = self.id().saturating_add(4);
        let num_partitions = self.num_partitions() as u32;

        // Sample the RDD to determine range bounds for proper partitioning
        let sample_size = (num_partitions as usize * 20).max(100); // 20 samples per partition, minimum 100
        let parent_rdd: Arc<dyn crate::traits::RddBase<Item = (String, i32)>> = self.clone();
        let sample_keys = crate::rdd::sorted_rdd::sample_keys_for_sorting(&parent_rdd, sample_size);

        let partitioner = Arc::new(crate::shuffle::RangePartitioner::from_sample(
            num_partitions,
            sample_keys,
        ));

        Arc::new(SortedRdd::new(new_rdd_id, self, partitioner, ascending))
    }

    fn cogroup<W: Data>(
        self: Arc<Self>,
        other: Arc<dyn RddBase<Item = (String, W)>>,
        partitioner: Arc<dyn Partitioner<String>>,
    ) -> Arc<CogroupedRdd<String, i32, W>> {
        let new_rdd_id = self.id().saturating_add(other.id()).saturating_add(2);
        Arc::new(CogroupedRdd::new(new_rdd_id, self, other, partitioner))
    }

    fn combine_by_key<C: Data>(
        self: Arc<Self>,
        aggregator: Arc<dyn Aggregator<String, i32, C>>,
        partitioner: Arc<dyn Partitioner<String>>,
    ) -> Arc<ShuffledRdd<String, i32, C>> {
        let new_rdd_id = self.id().saturating_add(3);
        Arc::new(ShuffledRdd::new(new_rdd_id, self, aggregator, partitioner))
    }

    /// This is the safe implementation for creating ShuffleMapTasks.
    /// It is only callable on a DistributedRdd<(K, V)>, so K and V are known at compile time.
    fn create_shuffle_map_tasks(
        &self,
        shuffle_id: u32,
        num_partitions: u32,
    ) -> RddResult<Vec<Box<dyn crate::distributed::task::Task>>> {
        let (base_data, num_base_partitions, operations) = self.clone().analyze_lineage();

        let mut tasks: Vec<Box<dyn crate::distributed::task::Task>> = Vec::new();
        for partition_index in 0..num_base_partitions {
            let data_len = base_data.len();
            let partition_size = data_len.div_ceil(num_base_partitions);
            let start = partition_index * partition_size;
            let end = std::cmp::min(start + partition_size, data_len);

            let partition_data = if start >= data_len {
                Vec::new()
            } else {
                base_data[start..end].to_vec()
            };

            let serialized_partition_data =
                bincode::encode_to_vec(&partition_data, bincode::config::standard())
                    .map_err(|e| crate::traits::RddError::SerializationError(e.to_string()))?;

            // Create the task safely, without transmute!
            let task = crate::distributed::task::ShuffleMapTask::<(String, i32)>::new(
                serialized_partition_data,
                operations.clone(),
                shuffle_id,
                num_partitions,
                Default::default(),
            );
            tasks.push(Box::new(task));
        }
        Ok(tasks)
    }
}

impl PairRdd for crate::rdd::DistributedRdd<(i32, String)> {
    fn reduce_by_key(
        self: Arc<Self>,
        reduce_func: fn(String, String) -> String,
        partitioner: Arc<dyn Partitioner<i32>>,
    ) -> Arc<ShuffledRdd<i32, String, String>> {
        let aggregator = Arc::new(ReduceAggregator::new(reduce_func));
        let new_rdd_id = self.id().saturating_add(1);
        Arc::new(ShuffledRdd::new(new_rdd_id, self, aggregator, partitioner))
    }

    fn group_by_key(
        self: Arc<Self>,
        partitioner: Arc<dyn Partitioner<i32>>,
    ) -> Arc<ShuffledRdd<i32, String, Vec<String>>> {
        let aggregator = Arc::new(crate::shuffle::GroupByKeyAggregator::new());
        let new_rdd_id = self.id().saturating_add(2);
        Arc::new(ShuffledRdd::new(new_rdd_id, self, aggregator, partitioner))
    }

    fn join<W: Data>(
        self: Arc<Self>,
        other: Arc<dyn RddBase<Item = (i32, W)>>,
        partitioner: Arc<dyn Partitioner<i32>>,
    ) -> Arc<JoinedRdd<i32, String, W>> {
        let new_rdd_id = self.id().saturating_add(other.id()).saturating_add(1);
        Arc::new(JoinedRdd::new(new_rdd_id, self, other, partitioner))
    }

    fn sort_by_key(self: Arc<Self>, ascending: bool) -> Arc<SortedRdd<i32, String>> {
        let new_rdd_id = self.id().saturating_add(4);
        let num_partitions = self.num_partitions() as u32;

        let sample_size = (num_partitions as usize * 20).max(100);
        let parent_rdd: Arc<dyn crate::traits::RddBase<Item = (i32, String)>> = self.clone();
        let sample_keys = crate::rdd::sorted_rdd::sample_keys_for_sorting(&parent_rdd, sample_size);

        let partitioner = Arc::new(crate::shuffle::RangePartitioner::from_sample(
            num_partitions,
            sample_keys,
        ));

        Arc::new(SortedRdd::new(new_rdd_id, self, partitioner, ascending))
    }

    fn cogroup<W: Data>(
        self: Arc<Self>,
        other: Arc<dyn RddBase<Item = (i32, W)>>,
        partitioner: Arc<dyn Partitioner<i32>>,
    ) -> Arc<CogroupedRdd<i32, String, W>> {
        let new_rdd_id = self.id().saturating_add(other.id()).saturating_add(2);
        Arc::new(CogroupedRdd::new(new_rdd_id, self, other, partitioner))
    }

    fn combine_by_key<C: Data>(
        self: Arc<Self>,
        aggregator: Arc<dyn Aggregator<i32, String, C>>,
        partitioner: Arc<dyn Partitioner<i32>>,
    ) -> Arc<ShuffledRdd<i32, String, C>> {
        let new_rdd_id = self.id().saturating_add(3);
        Arc::new(ShuffledRdd::new(new_rdd_id, self, aggregator, partitioner))
    }

    fn create_shuffle_map_tasks(
        &self,
        shuffle_id: u32,
        num_partitions: u32,
    ) -> RddResult<Vec<Box<dyn crate::distributed::task::Task>>> {
        let (base_data, num_base_partitions, operations) = self.clone().analyze_lineage();

        let mut tasks: Vec<Box<dyn crate::distributed::task::Task>> = Vec::new();
        for partition_index in 0..num_base_partitions {
            let data_len = base_data.len();
            let partition_size = data_len.div_ceil(num_base_partitions);
            let start = partition_index * partition_size;
            let end = std::cmp::min(start + partition_size, data_len);

            let partition_data = if start >= data_len {
                Vec::new()
            } else {
                base_data[start..end].to_vec()
            };

            let serialized_partition_data =
                bincode::encode_to_vec(&partition_data, bincode::config::standard())
                    .map_err(|e| crate::traits::RddError::SerializationError(e.to_string()))?;

            let task = crate::distributed::task::ShuffleMapTask::<(i32, String)>::new(
                serialized_partition_data,
                operations.clone(),
                shuffle_id,
                num_partitions,
                Default::default(),
            );
            tasks.push(Box::new(task));
        }
        Ok(tasks)
    }
}

/// Extension trait to provide convenient methods that automatically wrap RDDs in Arc
pub trait PairRddExt<K: Data, V: Data> {
    /// Groups values by key and applies a reduction function.
    fn reduce_by_key(
        self,
        reduce_func: fn(V, V) -> V,
        partitioner: Arc<dyn Partitioner<K>>,
    ) -> Arc<ShuffledRdd<K, V, V>>;

    /// Groups all values for a key into a single sequence.
    fn group_by_key(self, partitioner: Arc<dyn Partitioner<K>>) -> Arc<ShuffledRdd<K, V, Vec<V>>>;

    /// Return an RDD containing all pairs of elements with matching keys in `self` and `other`.
    fn join<W: Data>(
        self,
        other: Arc<dyn RddBase<Item = (K, W)>>,
        partitioner: Arc<dyn Partitioner<K>>,
    ) -> Arc<JoinedRdd<K, V, W>>;

    /// Return an RDD with the elements sorted by key.
    fn sort_by_key(self, ascending: bool) -> Arc<SortedRdd<K, V>>
    where
        K: Ord + std::fmt::Debug;

    /// Return an RDD that groups data from both RDDs by key.
    fn cogroup<W: Data>(
        self,
        other: Arc<dyn RddBase<Item = (K, W)>>,
        partitioner: Arc<dyn Partitioner<K>>,
    ) -> Arc<CogroupedRdd<K, V, W>>;

    /// Combine values with the same key using a custom aggregator.
    fn combine_by_key<C: Data>(
        self,
        aggregator: Arc<dyn Aggregator<K, V, C>>,
        partitioner: Arc<dyn Partitioner<K>>,
    ) -> Arc<ShuffledRdd<K, V, C>>;
}

impl PairRddExt<String, i32> for crate::rdd::DistributedRdd<(String, i32)> {
    fn reduce_by_key(
        self,
        reduce_func: fn(i32, i32) -> i32,
        partitioner: Arc<dyn Partitioner<String>>,
    ) -> Arc<ShuffledRdd<String, i32, i32>> {
        Arc::new(self).reduce_by_key(reduce_func, partitioner)
    }

    fn group_by_key(
        self,
        partitioner: Arc<dyn Partitioner<String>>,
    ) -> Arc<ShuffledRdd<String, i32, Vec<i32>>> {
        Arc::new(self).group_by_key(partitioner)
    }

    fn join<W: Data>(
        self,
        other: Arc<dyn RddBase<Item = (String, W)>>,
        partitioner: Arc<dyn Partitioner<String>>,
    ) -> Arc<JoinedRdd<String, i32, W>> {
        Arc::new(self).join(other, partitioner)
    }

    fn sort_by_key(self, ascending: bool) -> Arc<SortedRdd<String, i32>> {
        Arc::new(self).sort_by_key(ascending)
    }

    fn cogroup<W: Data>(
        self,
        other: Arc<dyn RddBase<Item = (String, W)>>,
        partitioner: Arc<dyn Partitioner<String>>,
    ) -> Arc<CogroupedRdd<String, i32, W>> {
        Arc::new(self).cogroup(other, partitioner)
    }

    fn combine_by_key<C: Data>(
        self,
        aggregator: Arc<dyn Aggregator<String, i32, C>>,
        partitioner: Arc<dyn Partitioner<String>>,
    ) -> Arc<ShuffledRdd<String, i32, C>> {
        Arc::new(self).combine_by_key(aggregator, partitioner)
    }
}

impl PairRddExt<i32, String> for crate::rdd::DistributedRdd<(i32, String)> {
    fn reduce_by_key(
        self,
        reduce_func: fn(String, String) -> String,
        partitioner: Arc<dyn Partitioner<i32>>,
    ) -> Arc<ShuffledRdd<i32, String, String>> {
        Arc::new(self).reduce_by_key(reduce_func, partitioner)
    }

    fn group_by_key(
        self,
        partitioner: Arc<dyn Partitioner<i32>>,
    ) -> Arc<ShuffledRdd<i32, String, Vec<String>>> {
        Arc::new(self).group_by_key(partitioner)
    }

    fn join<W: Data>(
        self,
        other: Arc<dyn RddBase<Item = (i32, W)>>,
        partitioner: Arc<dyn Partitioner<i32>>,
    ) -> Arc<JoinedRdd<i32, String, W>> {
        Arc::new(self).join(other, partitioner)
    }

    fn sort_by_key(self, ascending: bool) -> Arc<SortedRdd<i32, String>> {
        Arc::new(self).sort_by_key(ascending)
    }

    fn cogroup<W: Data>(
        self,
        other: Arc<dyn RddBase<Item = (i32, W)>>,
        partitioner: Arc<dyn Partitioner<i32>>,
    ) -> Arc<CogroupedRdd<i32, String, W>> {
        Arc::new(self).cogroup(other, partitioner)
    }

    fn combine_by_key<C: Data>(
        self,
        aggregator: Arc<dyn Aggregator<i32, String, C>>,
        partitioner: Arc<dyn Partitioner<i32>>,
    ) -> Arc<ShuffledRdd<i32, String, C>> {
        Arc::new(self).combine_by_key(aggregator, partitioner)
    }
}
