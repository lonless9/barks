//! Comprehensive tests for all shuffle functionality
//!
//! This test suite verifies:
//! - All partitioner implementations
//! - All aggregator implementations
//! - RDD shuffle operations (join, cogroup, sort)
//! - Shuffle optimizations and configurations
//! - Stage management for shuffle operations

use barks_core::rdd::transformations::PairRdd;
use barks_core::rdd::{CogroupedRdd, JoinedRdd, SortedRdd};
use barks_core::shuffle::{
    AverageAggregator, CountAggregator, CustomPartitioner, HashPartitioner, Partitioner,
    RangePartitioner, ReduceAggregator, SumAggregator,
};
use barks_core::traits::RddBase;
use barks_network_shuffle::{
    CompressionCodec, OptimizedShuffleBlockManager, ShuffleBlockManager, ShuffleConfig,
};
use std::sync::Arc;
use tempfile::TempDir;

#[test]
fn test_hash_partitioner() {
    let partitioner = HashPartitioner::new(4);
    assert_eq!(partitioner.num_partitions(), 4);

    // Test with seed
    let partitioner_with_seed = HashPartitioner::with_seed(4, 12345);
    assert_eq!(partitioner_with_seed.num_partitions(), 4);

    // Test partitioning consistency
    let key = "test_key";
    let partition1 = partitioner.get_partition_for(&key);
    let partition2 = partitioner.get_partition_for(&key);
    assert_eq!(partition1, partition2, "Partitioning should be consistent");
    assert!(partition1 < 4, "Partition should be within bounds");
}

#[test]
fn test_range_partitioner() {
    // Test with explicit bounds
    let bounds = vec![10, 20, 30];
    let partitioner = RangePartitioner::new(4, bounds);
    assert_eq!(partitioner.num_partitions(), 4);

    // Test partitioning
    assert_eq!(partitioner.get_partition_for(&5), 0);
    assert_eq!(partitioner.get_partition_for(&15), 1);
    assert_eq!(partitioner.get_partition_for(&25), 2);
    assert_eq!(partitioner.get_partition_for(&35), 3);

    // Test from sample
    let sample = vec![1, 5, 10, 15, 20, 25, 30, 35, 40];
    let sample_partitioner = RangePartitioner::from_sample(3, sample);
    assert_eq!(sample_partitioner.num_partitions(), 3);
}

#[test]
fn test_custom_partitioner() {
    let partitioner = CustomPartitioner::new(3, |key: &i32| {
        if *key < 10 {
            0
        } else if *key < 20 {
            1
        } else {
            2
        }
    });

    assert_eq!(partitioner.num_partitions(), 3);
    assert_eq!(partitioner.get_partition_for(&5), 0);
    assert_eq!(partitioner.get_partition_for(&15), 1);
    assert_eq!(partitioner.get_partition_for(&25), 2);

    // Test modulo behavior for out-of-range values
    let large_partitioner = CustomPartitioner::new(2, |_: &i32| 5);
    assert_eq!(large_partitioner.get_partition_for(&1), 1); // 5 % 2 = 1
}

#[test]
fn test_aggregator_creation() {
    // Test that aggregators can be created successfully
    let _sum_agg = SumAggregator::<i32>::new();
    let _count_agg = CountAggregator::<String>::new();
    let _avg_agg = AverageAggregator::<i32>::new();
    let _reduce_agg = ReduceAggregator::new(|a: i32, b: i32| a + b);

    // Test that they implement the required traits
    assert!(true); // Compilation success is the test
}

// Note: PairRdd operations are not fully implemented yet for DistributedRdd
// These tests are commented out until shuffle operations are implemented
/*
#[test]
fn test_pair_rdd_operations() {
    let data = vec![
        ("a".to_string(), 1),
        ("b".to_string(), 2),
        ("a".to_string(), 3),
        ("c".to_string(), 4),
    ];
    let rdd = DistributedRdd::from_vec_with_partitions(data, 2);
    let partitioner = Arc::new(HashPartitioner::new(3));

    // Test reduce_by_key
    let reduced_rdd = rdd.clone().reduce_by_key(|a, b| a + b, partitioner.clone());
    assert_eq!(reduced_rdd.num_partitions(), 3);

    // Test group_by_key
    let grouped_rdd = rdd.clone().group_by_key(partitioner.clone());
    assert_eq!(grouped_rdd.num_partitions(), 3);

    // Test combine_by_key
    let sum_aggregator = Arc::new(SumAggregator::new());
    let combined_rdd = rdd.combine_by_key(sum_aggregator, partitioner);
    assert_eq!(combined_rdd.num_partitions(), 3);
}
*/

#[test]
fn test_join_operations() {
    let left_data = vec![
        ("a".to_string(), 1),
        ("b".to_string(), 2),
        ("c".to_string(), 3),
    ];
    let right_data = vec![
        ("a".to_string(), "apple".to_string()),
        ("b".to_string(), "banana".to_string()),
        ("d".to_string(), "date".to_string()),
    ];

    let left_rdd = SimpleRdd::from_vec_with_partitions(left_data, 2);
    let right_rdd = Arc::new(SimpleRdd::from_vec_with_partitions(right_data, 2));
    let partitioner = Arc::new(HashPartitioner::new(3));

    // Test join
    let joined_rdd = left_rdd
        .clone()
        .join(right_rdd.clone(), partitioner.clone());
    assert_eq!(joined_rdd.num_partitions(), 3);
    assert_eq!(joined_rdd.dependencies().len(), 2); // Two shuffle dependencies

    // Test cogroup
    let cogrouped_rdd = left_rdd.cogroup(right_rdd, partitioner);
    assert_eq!(cogrouped_rdd.num_partitions(), 3);
    assert_eq!(cogrouped_rdd.dependencies().len(), 2); // Two shuffle dependencies
}

#[test]
fn test_sort_operations() {
    let data = vec![
        ("zebra".to_string(), 1),
        ("apple".to_string(), 2),
        ("banana".to_string(), 3),
    ];
    let rdd = SimpleRdd::from_vec_with_partitions(data, 2);

    // Test sort_by_key
    let sorted_rdd = rdd.sort_by_key(true);
    assert!(sorted_rdd.num_partitions() > 0);
    assert_eq!(sorted_rdd.dependencies().len(), 1); // One shuffle dependency
}

#[test]
fn test_shuffle_config() {
    let default_config = ShuffleConfig::default();
    assert_eq!(default_config.compression, CompressionCodec::None);
    assert_eq!(default_config.spill_threshold, 64 * 1024 * 1024);
    assert!(default_config.sort_based_shuffle);
    assert_eq!(default_config.io_buffer_size, 64 * 1024);

    let custom_config = ShuffleConfig {
        compression: CompressionCodec::None,
        spill_threshold: 1024 * 1024,
        sort_based_shuffle: false,
        io_buffer_size: 8192,
    };
    assert_eq!(custom_config.spill_threshold, 1024 * 1024);
    assert!(!custom_config.sort_based_shuffle);
}

#[test]
fn test_compression_codec() {
    let codec = CompressionCodec::None;
    let data = b"test data for compression";

    let compressed = codec.compress(data).unwrap();
    assert_eq!(compressed, data); // No compression should return original data

    let decompressed = codec.decompress(&compressed).unwrap();
    assert_eq!(decompressed, data);
}

#[tokio::test]
async fn test_optimized_shuffle_block_manager() {
    let temp_dir = TempDir::new().unwrap();
    let config = ShuffleConfig::default();
    let manager = OptimizedShuffleBlockManager::new(temp_dir.path(), config).unwrap();

    let block_id = barks_network_shuffle::ShuffleBlockId {
        shuffle_id: 1,
        map_id: 0,
        reduce_id: 0,
    };
    let test_data = b"test shuffle block data".to_vec();

    // Test put and get
    manager
        .put_block(block_id.clone(), test_data.clone())
        .await
        .unwrap();
    assert!(manager.contains_block(&block_id).await.unwrap());

    let retrieved_data = manager.get_block(&block_id).await.unwrap();
    assert_eq!(retrieved_data, test_data);

    // Test block size
    let size = manager.get_block_size(&block_id).await.unwrap();
    assert!(size > 0);

    // Test remove
    manager.remove_block(&block_id).await.unwrap();
    assert!(!manager.contains_block(&block_id).await.unwrap());
}

#[test]
fn test_rdd_type_consistency() {
    // Test that all RDD types implement the required traits
    let data = vec![("key".to_string(), 1)];
    let rdd = SimpleRdd::from_vec_with_partitions(data, 1);
    let partitioner = Arc::new(HashPartitioner::new(2));

    // These should compile without issues
    let _: JoinedRdd<String, i32, String> = rdd.clone().join(
        Arc::new(SimpleRdd::from_vec_with_partitions(
            vec![("key".to_string(), "value".to_string())],
            1,
        )),
        partitioner.clone(),
    );

    let _: CogroupedRdd<String, i32, String> = rdd.clone().cogroup(
        Arc::new(SimpleRdd::from_vec_with_partitions(
            vec![("key".to_string(), "value".to_string())],
            1,
        )),
        partitioner,
    );

    let _: SortedRdd<String, i32> = rdd.sort_by_key(true);
}

#[test]
fn test_partitioner_bounds() {
    // Test that partitioners handle edge cases correctly
    let hash_partitioner = HashPartitioner::new(1);
    assert_eq!(hash_partitioner.get_partition_for(&"any_key"), 0);

    let range_partitioner = RangePartitioner::from_sample(1, vec![1, 2, 3]);
    assert_eq!(range_partitioner.get_partition_for(&0), 0);
    assert_eq!(range_partitioner.get_partition_for(&100), 0);

    // Test empty sample
    let empty_range_partitioner: RangePartitioner<i32> = RangePartitioner::from_sample(3, vec![]);
    assert_eq!(empty_range_partitioner.num_partitions(), 3);
}
